# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l5WwQOWiJggWp-rmdrhmwdJbDVT9YOXx
"""

import pandas as pd
import numpy as np
import re
import joblib
from scipy.sparse import hstack, csr_matrix
import streamlit as st
import tempfile
import os
import pickle

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞", page_icon="üìù", layout="wide")
st.title("üìù –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å—Ç–Ω–æ–≥–æ —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É")
st.write("–ó–∞–≥—Ä—É–∑–∏ CSV —Ñ–∞–π–ª —Å —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–æ–ª—É—á–∏ –æ—Ü–µ–Ω–∫–∏")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø—É—Ç–µ–π ===
@st.cache_resource
def load_models():
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        model_files = [
            'tfidf_vectorizer.pkl',
            'feature_scaler.pkl',
            'model_q1.pkl',
            'model_q2.pkl',
            'model_q3.pkl',
            'model_q4_enhanced.pkl',
            'tfidf_q4.pkl',
            'scaler_q4.pkl'
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        missing_files = []
        for file in model_files:
            if not os.path.exists(file):
                missing_files.append(file)

        if missing_files:
            st.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π: {missing_files}")
            st.info("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ")
            return None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º tfidf_vectorizer.pkl...")
        tfidf = joblib.load('tfidf_vectorizer.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º feature_scaler.pkl...")
        scaler = joblib.load('feature_scaler.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º model_q1.pkl...")
        model_q1 = joblib.load('model_q1.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º model_q2.pkl...")
        model_q2 = joblib.load('model_q2.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º model_q3.pkl...")
        model_q3 = joblib.load('model_q3.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º model_q4_enhanced.pkl...")
        model_q4 = joblib.load('model_q4_enhanced.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º tfidf_q4.pkl...")
        tfidf_q4 = joblib.load('tfidf_q4.pkl')

        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º scaler_q4.pkl...")
        scaler_q4 = joblib.load('scaler_q4.pkl')

        return tfidf, scaler, model_q1, model_q2, model_q3, model_q4, tfidf_q4, scaler_q4

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        import traceback
        st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
        return None

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π
with st.spinner('–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥'):
    models = load_models()

if models is None:
    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã .pkl –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
    st.stop()

tfidf, scaler, model_q1, model_q2, model_q3, model_q4, tfidf_q4, scaler_q4 = models
st.success("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

# === –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç–≤–æ–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏) ===
def clean_html(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, ' ', str(text)).replace('  ', ' ').strip()

def remove_instruction(transcript, q_num):
    if q_num == 1: start_phrase = "–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å–≤–æ–π –¥–∏–∞–ª–æ–≥."
    elif q_num == 2: start_phrase = "–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞ –ø–æ–ª–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏."
    elif q_num == 3: start_phrase = "–ü–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç–µ –∑–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
    elif q_num == 4: start_phrase = "–ö–æ–≥–¥–∞ –±—É–¥–µ—Ç–µ –≥–æ—Ç–æ–≤—ã, –º–æ–∂–µ—Ç–µ –Ω–∞—á–∏–Ω–∞—Ç—å –æ–ø–∏—Å—ã–≤–∞—Ç—å."
    else: return transcript
    idx = transcript.find(start_phrase)
    return transcript[idx + len(start_phrase):].strip() if idx != -1 else transcript

def extract_features(text):
    sentences = re.split(r'[.!?]+', text)
    n_sents = len([s for s in sentences if len(s.strip()) > 0])
    words = text.split()
    n_words = len(words)
    avg_sent_len = n_words / n_sents if n_sents > 0 else 0
    return [n_sents, n_words, avg_sent_len, int('?' in text)]

def get_q4_features_enhanced(text):
    text_low = text.lower()
    return {
        'has_season': int(any(w in text_low for w in ['–ª–µ—Ç–æ', '–∑–∏–º–∞', '–≤–µ—Å–Ω–∞', '–æ—Å–µ–Ω—å', '—Ç—ë–ø–ª–æ–µ –≤—Ä–µ–º—è', '—Å–Ω–µ–≥', '–¥–æ–∂–¥—å'])),
        'has_place': int(any(w in text_low for w in ['–∫—É—Ö–Ω—è', '–¥–æ–º', '–ø–∞—Ä–∫', '–≤–æ–∫–∑–∞–ª', '—Ä–µ–∫–∞', '—É–ª–∏—Ü–∞'])),
        'has_people_count': int(any(w in text_low for w in ['–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–º–Ω–æ–≥–æ –¥–µ—Ç–µ–π', '—Ü–µ–ª–∞—è —Å–µ–º—å—è'])),
        'has_family': int(any(w in text_low for w in ['–≤ –Ω–∞—à–µ–π —Å–µ–º—å–µ', '—É –º–µ–Ω—è —Ç—Ä–æ–µ –¥–µ—Ç–µ–π', '—è —Å—Ç–∞—Ä—à–∞—è', '–º–æ–π –±—Ä–∞—Ç'])),
        'has_hobby': int(any(w in text_low for w in ['–ª—é–±–ª—é –≥–æ—Ç–æ–≤–∏—Ç—å', '–∏–≥—Ä–∞—é –≤ —Ñ—É—Ç–±–æ–ª', '–≥—É–ª—è—é –Ω–∞ –ø—Ä–∏—Ä–æ–¥–µ', '–≤—ã—à–∏–≤–∞—é'])),
        'n_sentences': len(re.split(r'[.!?]+', text)),
        'is_structured': int(len(re.findall(r'\b(–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ|–∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ|—è –≤–∏–∂—É|—Ä–∞—Å—Å–∫–∞–∂—É –æ)\b', text_low)) >= 1),
        'has_emotion': int(any(w in text_low for w in ['—Ä–∞–¥–æ—Å—Ç–Ω—ã–π', '—Å—á–∞—Å—Ç–ª–∏–≤', '—É–ª—ã–±–∞–µ—Ç—Å—è', '–≤–µ—Å–µ–ª–æ'])),
        'is_garbage': int(any(w in text_low for w in [
            'characterization', 'leather.ru', 'Feit', '–ü–∞—Å–ø–æ—Ä—Ç–Ω—ã–π –∫–∞–Ω–∞–ª', 'understanding'
        ]) or len(text.split()) < 3)
    }

# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ ===
def grade_exam(uploaded_file):
    try:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        df = pd.read_csv(uploaded_file, sep=';', on_bad_lines='skip')

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress_bar = st.progress(0)
        status_text = st.empty()

        status_text.text("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
        progress_bar.progress(10)

        if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' in df.columns:
            df = df.drop(columns=['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'])

        required_cols = ['Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞',
                         '–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', '–°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –∑–∞–ø–∏—Å']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞: {col}")

        status_text.text("üßπ –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç...")
        progress_bar.progress(30)

        df['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞_clean'] = df['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞'].apply(clean_html)
        df['cleaned_transcript'] = df.apply(
            lambda row: remove_instruction(row['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞'], row['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']),
            axis=1
        )
        df['combined_text'] = df['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞_clean'] + ' [SEP] ' + df['cleaned_transcript'].fillna('')

        status_text.text("üìä –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏...")
        progress_bar.progress(60)

        y_pred = np.zeros(len(df), dtype=int)

        for q_num in [1, 2, 3, 4]:
            mask = df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == q_num
            if not mask.any():
                continue

            if q_num == 4:
                X_text = tfidf_q4.transform(df.loc[mask, 'combined_text'])
                ling_feat = np.array([extract_features(txt) for txt in df.loc[mask, 'cleaned_transcript'].fillna('')])
                ling_scaled = scaler_q4.transform(ling_feat)
                feats = df.loc[mask, 'cleaned_transcript'].apply(get_q4_features_enhanced)
                feature_cols = list(feats.iloc[0].keys())
                checklist_feat = np.array([list(f.values()) for f in feats])
                X = hstack([X_text, csr_matrix(ling_scaled), csr_matrix(checklist_feat)])
                pred_raw = model_q4.predict(X)
                pred_rounded = np.array([int(np.clip(round(p), 0, 2)) for p in pred_raw])
                pred_rounded[checklist_feat[:, -1] == 1] = 0
                y_pred[mask] = pred_rounded
            else:
                X_text = tfidf.transform(df.loc[mask, 'combined_text'])
                ling_feat = np.array([extract_features(txt) for txt in df.loc[mask, 'cleaned_transcript'].fillna('')])
                ling_scaled = scaler.transform(ling_feat)
                q_norm = np.full((mask.sum(), 1), q_num / 4.0)
                X = hstack([X_text, csr_matrix(q_norm), csr_matrix(ling_scaled)])
                model = {1: model_q1, 2: model_q2, 3: model_q3}[q_num]
                pred_raw = model.predict(X)
                if q_num in (1, 3):
                    pred_rounded = np.array([0 if p < 0.5 else 1 for p in pred_raw])
                else:
                    pred_rounded = np.array([int(np.clip(round(p), 0, 2)) for p in pred_raw])
                y_pred[mask] = pred_rounded

        df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'] = y_pred
        output_cols = ['Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞',
                       '–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞', '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞',
                       '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', '–°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –∑–∞–ø–∏—Å']
        df = df[output_cols]

        status_text.text("‚úÖ –ì–æ—Ç–æ–≤–æ!")
        progress_bar.progress(100)

        return df

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        import traceback
        st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
        return None

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞–≥—Ä—É–∑–∫–∏ ===
st.header("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞")

uploaded_file = st.file_uploader(
    "–í—ã–±–µ—Ä–∏ CSV —Ñ–∞–π–ª —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'",
    type=['csv'],
    help="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: Id —ç–∫–∑–∞–º–µ–Ω–∞, Id –≤–æ–ø—Ä–æ—Å–∞, ‚Ññ –≤–æ–ø—Ä–æ—Å–∞, –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞, –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∏ –¥—Ä."
)

if uploaded_file is not None:
    st.success(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {uploaded_file.name}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ
    file_details = {
        "–ò–º—è —Ñ–∞–π–ª–∞": uploaded_file.name,
        "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞": f"{uploaded_file.size / 1024:.2f} KB",
        "–¢–∏–ø —Ñ–∞–π–ª–∞": uploaded_file.type
    }
    st.json(file_details)

    if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ—Ü–µ–Ω–∫—É", type="primary"):
        with st.spinner('–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞'):
            result_df = grade_exam(uploaded_file)

        if result_df is not None:
            st.success("üéâ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            st.subheader("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("–í—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤", len(result_df))
            with col2:
                st.metric("–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞", f"{result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].mean():.2f}")
            with col3:
                st.metric("–ú–∞–∫—Å –æ—Ü–µ–Ω–∫–∞", result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].max())
            with col4:
                st.metric("–ú–∏–Ω –æ—Ü–µ–Ω–∫–∞", result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].min())

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
            st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫")
            grade_dist = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].value_counts().sort_index()
            st.bar_chart(grade_dist)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–µ—Ä–≤—ã–µ 10 –∑–∞–ø–∏—Å–µ–π)")
            st.dataframe(result_df.head(10))

            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
            st.subheader("üìù –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º")
            question_stats = result_df.groupby('‚Ññ –≤–æ–ø—Ä–æ—Å–∞')['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].agg(['count', 'mean', 'min', 'max']).round(2)
            st.dataframe(question_stats)

            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            st.subheader("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            csv = result_df.to_csv(index=False, sep=';', encoding='utf-8-sig')
            st.download_button(
                label="üíæ –°–∫–∞—á–∞—Ç—å CSV —Å –æ—Ü–µ–Ω–∫–∞–º–∏",
                data=csv,
                file_name="graded_exam_results.csv",
                mime="text/csv",
                help="–§–∞–π–ª –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'"
            )

# === –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–∞–Ω–µ–ª—å ===
st.sidebar.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
st.sidebar.info("""
**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:**
1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ —ç–∫–∑–∞–º–µ–Ω–∞
2. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–ù–∞—á–∞—Ç—å –æ—Ü–µ–Ω–∫—É'
3. –î–æ–∂–¥–∏—Ç–µ—Å—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
4. –°–∫–∞—á–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ–∞–π–ª—É:**
- –§–æ—Ä–º–∞—Ç: CSV —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'
- –ö–æ–¥–∏—Ä–æ–≤–∫–∞: UTF-8
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: Id —ç–∫–∑–∞–º–µ–Ω–∞, Id –≤–æ–ø—Ä–æ—Å–∞, ‚Ññ –≤–æ–ø—Ä–æ—Å–∞, –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞, –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
""")

st.sidebar.header("üìä –ú–æ–¥–µ–ª–∏")
st.sidebar.success("""
–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:
- TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
- Scaler –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –ú–æ–¥–µ–ª–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ 1-4
- –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ 4
""")

st.info("üí° MAE = 0.0397 ‚Ä¢ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';' –∏ –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π UTF-8")

# === –§—É—Ç–µ—Ä ===
st.markdown("---")
st.markdown("### üöÄ –†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ –Ω–∞ Render | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–æ–≤")